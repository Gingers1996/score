#!/usr/bin/env python3
"""
æµ‹è¯• app_cloud_safe.py çš„æ ¸å¿ƒåŠŸèƒ½
"""

import pandas as pd
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥åº”ç”¨ä¸­çš„å‡½æ•°
from app_cloud_safe import calculate_total_score, process_data, assign_grades, validate_cutoff_input

def test_calculate_total_score():
    """æµ‹è¯•æ€»åˆ†è®¡ç®—åŠŸèƒ½"""
    print("ğŸ§® æµ‹è¯•æ€»åˆ†è®¡ç®—...")
    
    # æµ‹è¯•æ•°æ®
    test_data = [
        {'ç”²éƒ¨åˆ†æ•°': 45, 'ä¹™éƒ¨åˆ†æ•°': 95},
        {'ç”²éƒ¨åˆ†æ•°': 42, 'ä¹™éƒ¨åˆ†æ•°': 88},
        {'ç”²éƒ¨åˆ†æ•°': 48, 'ä¹™éƒ¨åˆ†æ•°': 92},
        {'ç”²éƒ¨åˆ†æ•°': 0, 'ä¹™éƒ¨åˆ†æ•°': 0},  # æµ‹è¯•è¾¹ç•Œå€¼
        {'ç”²éƒ¨åˆ†æ•°': 50, 'ä¹™éƒ¨åˆ†æ•°': 103}  # æµ‹è¯•æ»¡åˆ†
    ]
    
    expected_scores = [92, 85, 91, 0, 100]  # é¢„æœŸç»“æœ
    
    for i, (data, expected) in enumerate(zip(test_data, expected_scores)):
        result = calculate_total_score(data)
        status = "âœ…" if result == expected else "âŒ"
        print(f"  æµ‹è¯• {i+1}: {status} è¾“å…¥: ç”²éƒ¨={data['ç”²éƒ¨åˆ†æ•°']}, ä¹™éƒ¨={data['ä¹™éƒ¨åˆ†æ•°']} -> æ€»åˆ†={result} (æœŸæœ›: {expected})")
    
    print()

def test_process_data():
    """æµ‹è¯•æ•°æ®å¤„ç†åŠŸèƒ½"""
    print("ğŸ“Š æµ‹è¯•æ•°æ®å¤„ç†...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_df = pd.DataFrame({
        'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­'],
        'å­¦å·': ['001', '002', '003', '004'],
        'ç­çº§': ['ä¸€ç­', 'ä¸€ç­', 'äºŒç­', 'äºŒç­'],
        'ç”²éƒ¨åˆ†æ•°': [45, 42, 48, 40],
        'ä¹™éƒ¨åˆ†æ•°': [95, 88, 92, 85]
    })
    
    # å¤„ç†æ•°æ®
    processed_df = process_data(test_df)
    
    # éªŒè¯ç»“æœ
    print(f"  åŸå§‹æ•°æ®è¡Œæ•°: {len(test_df)}")
    print(f"  å¤„ç†åæ•°æ®è¡Œæ•°: {len(processed_df)}")
    print(f"  æ˜¯å¦åŒ…å«æ€»åˆ†åˆ—: {'æ€»åˆ†' in processed_df.columns}")
    print(f"  æ˜¯å¦åŒ…å«æ’ååˆ—: {'æ’å' in processed_df.columns}")
    print(f"  æ€»åˆ†èŒƒå›´: {processed_df['æ€»åˆ†'].min()} - {processed_df['æ€»åˆ†'].max()}")
    print(f"  æ’åèŒƒå›´: {processed_df['æ’å'].min()} - {processed_df['æ’å'].max()}")
    
    # éªŒè¯æ’åº
    is_sorted = processed_df['æ€»åˆ†'].is_monotonic_decreasing
    print(f"  æ˜¯å¦æŒ‰æ€»åˆ†é™åºæ’åˆ—: {'âœ…' if is_sorted else 'âŒ'}")
    
    print()

def test_assign_grades():
    """æµ‹è¯•ç­‰çº§åˆ†é…åŠŸèƒ½"""
    print("ğŸ† æµ‹è¯•ç­‰çº§åˆ†é…...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_df = pd.DataFrame({
        'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ'],
        'æ€»åˆ†': [75, 70, 77, 65, 80]
    })
    
    # æµ‹è¯•ç­‰çº§åˆ†æ•°çº¿
    cutoff_scores = {
        'Level2': 47,
        'Level3': 53,
        'Level4': 58,
        'Level5': 63,
        'Level6': 66,
        'Level7': 70
    }
    
    # åˆ†é…ç­‰çº§
    graded_df = assign_grades(test_df, cutoff_scores)
    
    # éªŒè¯ç»“æœ
    print(f"  åŸå§‹æ•°æ®è¡Œæ•°: {len(test_df)}")
    print(f"  åˆ†é…ç­‰çº§åè¡Œæ•°: {len(graded_df)}")
    print(f"  æ˜¯å¦åŒ…å«ç­‰çº§åˆ—: {'ç­‰çº§' in graded_df.columns}")
    
    # æ˜¾ç¤ºç­‰çº§åˆ†é…ç»“æœ
    print("  ç­‰çº§åˆ†é…ç»“æœ:")
    for _, row in graded_df.iterrows():
        print(f"    {row['å§“å']}: æ€»åˆ†={row['æ€»åˆ†']}, ç­‰çº§={row['ç­‰çº§']}")
    
    print()

def test_validate_cutoff_input():
    """æµ‹è¯•ç­‰çº§åˆ†æ•°çº¿è¾“å…¥éªŒè¯"""
    print("ğŸ”¢ æµ‹è¯•ç­‰çº§åˆ†æ•°çº¿è¾“å…¥éªŒè¯...")
    
    test_inputs = [
        ("47", 47),      # æœ‰æ•ˆæ•´æ•°
        ("53.5", None),  # å°æ•°ï¼ˆæ— æ•ˆï¼‰
        ("abc", None),   # éæ•°å­—ï¼ˆæ— æ•ˆï¼‰
        ("-5", None),    # è´Ÿæ•°ï¼ˆæ— æ•ˆï¼‰
        ("120", None),   # è¶…å‡ºèŒƒå›´ï¼ˆæ— æ•ˆï¼‰
        ("0", 0),        # è¾¹ç•Œå€¼
        ("100", 100),    # è¾¹ç•Œå€¼
    ]
    
    for input_val, expected in test_inputs:
        result = validate_cutoff_input(input_val)
        status = "âœ…" if result == expected else "âŒ"
        print(f"  è¾“å…¥ '{input_val}' -> {status} ç»“æœ: {result} (æœŸæœ›: {expected})")
    
    print()

def test_ranking_logic():
    """æµ‹è¯•æ’åé€»è¾‘ï¼ˆç›¸åŒåˆ†æ•°ç›¸åŒæ’åï¼‰"""
    print("ğŸ“ˆ æµ‹è¯•æ’åé€»è¾‘...")
    
    # åˆ›å»ºåŒ…å«ç›¸åŒåˆ†æ•°çš„æµ‹è¯•æ•°æ®
    test_df = pd.DataFrame({
        'å§“å': ['å¼ ä¸‰', 'æå››', 'ç‹äº”', 'èµµå…­', 'é’±ä¸ƒ', 'å­™å…«'],
        'å­¦å·': ['001', '002', '003', '004', '005', '006'],
        'ç­çº§': ['ä¸€ç­', 'ä¸€ç­', 'äºŒç­', 'äºŒç­', 'ä¸‰ç­', 'ä¸‰ç­'],
        'ç”²éƒ¨åˆ†æ•°': [48, 45, 46, 42, 47, 40],
        'ä¹™éƒ¨åˆ†æ•°': [96, 90, 91, 85, 93, 80]
    })
    
    # å¤„ç†æ•°æ®
    processed_df = process_data(test_df)
    
    print("  æ’åç»“æœ:")
    for _, row in processed_df.iterrows():
        print(f"    {row['å§“å']}: æ€»åˆ†={row['æ€»åˆ†']}, æ’å={row['æ’å']}")
    
    # éªŒè¯ç›¸åŒåˆ†æ•°æ˜¯å¦æœ‰ç›¸åŒæ’å
    score_rank_map = {}
    for _, row in processed_df.iterrows():
        score = row['æ€»åˆ†']
        rank = row['æ’å']
        if score in score_rank_map:
            if score_rank_map[score] != rank:
                print(f"  âŒ ç›¸åŒåˆ†æ•° {score} çš„æ’åä¸ä¸€è‡´: {score_rank_map[score]} vs {rank}")
                return
        else:
            score_rank_map[score] = rank
    
    print("  âœ… ç›¸åŒåˆ†æ•°è·å¾—ç›¸åŒæ’å")
    print()

def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯• app_cloud_safe.py æ ¸å¿ƒåŠŸèƒ½")
    print("=" * 50)
    
    try:
        test_calculate_total_score()
        test_process_data()
        test_assign_grades()
        test_validate_cutoff_input()
        test_ranking_logic()
        
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("âœ… åº”ç”¨æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        return False
    
    return True

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
